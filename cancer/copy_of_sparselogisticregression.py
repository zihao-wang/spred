# -*- coding: utf-8 -*-
"""Copy of SparseLogisticRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17OvtBhexVVIOnp--eqxiV8pt22qHetJ0

# MNIST with SciKit-Learn and skorch

This notebooks shows how to define and train a simple Neural-Network with PyTorch and use it via skorch with SciKit-Learn.

<table align="left"><td>
<a target="_blank" href="https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb">
    <img src="https://www.tensorflow.org/images/colab_logo_32px.png" />Run in Google Colab</a>  
</td><td>
<a target="_blank" href="https://github.com/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb"><img width=32px src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" />View source on GitHub</a></td></table>

**Note**: If you are running this in [a colab notebook](https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb), we recommend you enable a free GPU by going:

> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**

If you are running in colab, you should install the dependencies and download the dataset by running the following cell:
"""

! [ ! -z "$COLAB_GPU" ] && pip install torch scikit-learn==0.20.* skorch

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import warnings

"""## Loading Data
Using SciKit-Learns ```fetch_openml``` to load MNIST data.
"""

mnist = fetch_openml('mnist_784', cache=False)
mnist.data.shape

"""## Preprocessing Data

Each image of the MNIST dataset is encoded in a 784 dimensional vector, representing a 28 x 28 pixel image. Each pixel has a value between 0 and 255, corresponding to the grey-value of a pixel.<br />
The above ```featch_mldata``` method to load MNIST returns ```data``` and ```target``` as ```uint8``` which we convert to ```float32``` and ```int64``` respectively.

To avoid big weights that deal with the pixel values from between [0, 255], we scale `X` down. A commonly used range is [0, 1].
"""

X = mnist.data.astype('float32')
y = mnist.target.astype('int64')
X /= 255.0
X.min(), X.max()

"""Note: data is not normalized."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
assert(X_train.shape[0] + X_test.shape[0] == mnist.data.shape[0])
X_train.shape, y_train.shape

"""### Print a selection of training images and their labels"""

def plot_example(X, y):
    """Plot the first 5 images and their labels in a row."""
    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):
        plt.subplot(151 + i)
        plt.imshow(img)
        plt.xticks([])
        plt.yticks([])
        plt.title(y)

plot_example(X_train, y_train)

"""## Build Neural Network with PyTorch
Simple, fully connected neural network with one hidden layer. Input layer has 784 dimensions (28x28), hidden layer has 98 (= 784 / 8) and output layer 10 neurons, representing digits 0 - 9.
"""

import torch
from torch import nn
import torch.nn.functional as F
torch.manual_seed(0)
from skorch import NeuralNetClassifier

device = 'cuda' if torch.cuda.is_available() else 'cpu'

mnist_dim = X.shape[1]
hidden_dim = int(mnist_dim)
output_dim = len(np.unique(mnist.target))

mnist_dim, hidden_dim, output_dim

"""### Define neural networks in PyTorch's framework."""

class RedLinear(nn.Module):
    def __init__(self,in_dim,out_dim):            
        super(RedLinear, self).__init__()
        self.weight = nn.Parameter(torch.zeros([in_dim, out_dim]).normal_(0, 1/out_dim ** 0.5))
        self.weight2 = nn.Parameter(torch.zeros([in_dim, out_dim]).normal_(0, 1/out_dim ** 0.5))
        self.bias = nn.Parameter(torch.zeros([1, out_dim]).normal_(0, 1))


    def forward(self, X, **kwargs):
        X = (X).mm((self.weight * self.weight2)) + self.bias
        return X

    def get_weights(self):
        return {'weights': self.weight * self.weight2}
      

class SparseWeightNet(nn.Module):
    def __init__(
            self,
            input_dim=mnist_dim,
            hidden_dim=2000,
            output_dim=output_dim,
            dropout=0.5,
    ):
        super(SparseWeightNet, self).__init__()
        self.dropout = nn.Dropout(dropout)

        self.hidden = RedLinear(input_dim, hidden_dim)
        self.output = RedLinear(hidden_dim, output_dim)

    def forward(self, X, **kwargs):
        X = F.relu(self.hidden(X))
        #X = self.dropout(X)
        X = F.softmax(self.output(X), dim=-1)
        return X

    def get_weights(self):
        return {'hidden_weights': self.hidden.get_weights()['weights'],
                'output_weights': self.output.get_weights()['weights']}
    

class SparseFeatureNet(nn.Module):
    def __init__(
            self,
            input_dim=mnist_dim,
            hidden_dim=100,
            output_dim=output_dim,
    ):
        super(SparseFeatureNet, self).__init__()
        self.input_mask = nn.Parameter(torch.zeros([1, input_dim]).normal_(0, 1))
        self.hidden = nn.Linear(input_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, output_dim)

    def forward(self, X, **kwargs):
        X = F.relu(self.hidden(X * self.input_mask))
        X = F.softmax(self.output(X), dim=-1)
        return X
    
    def get_weights(self):
        return {'hidden_weights': self.hidden.weight * self.input_mask.T,
                'output_weights': self.output.weight}

class SparseFeatureLinear(nn.Module):
    def __init__(
            self,
            input_dim=mnist_dim,
            output_dim=output_dim):
        super(SparseFeatureLinear, self).__init__()
        self.input_mask = nn.Parameter(torch.zeros([1, input_dim]).normal_(0, 1))
        self.output = nn.Linear(input_dim, output_dim)

    def forward(self, X, **kwargs):
        X = (X * self.input_mask)
        #X = self.dropout(X)
        X = F.softmax(self.output(X), dim=-1)
        return X
    
    def get_weights(self):
        return {
            "output_weights": self.output.weight * self.input_mask
        }

class FNN(nn.Module):
    def __init__(
            self,
            input_dim=mnist_dim,
            hidden_dim=300,
            output_dim=output_dim,
            dropout=0.5,
    ):
        super(FNN, self).__init__()
        self.dropout = nn.Dropout(dropout)

        self.hidden = nn.Linear(input_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, output_dim)

    def forward(self, X, **kwargs):
        X = F.relu(self.hidden(X))
        #X = self.dropout(X)
        X = F.softmax(self.output(X), dim=-1)
        return X

    def get_weights(self):
        return {
            "output_weights": self.output.weight,
            "hidden_weights": self.hidden.weight,
        }

"""skorch allows to use PyTorch's networks in the SciKit-Learn setting:"""

def train_and_compress_ratio(NetworkClass, 
                             thr=1e-20,
                             max_epochs=2,
                             lr=0.004,
                             weight_decay=1e-5,
                             device=device,
                             ):
    net = NeuralNetClassifier(
        NetworkClass,
        max_epochs=max_epochs,
        lr=lr,
        device=device,
        optimizer=torch.optim.Adam,
        optimizer__weight_decay=weight_decay
    )
    warnings.filterwarnings('ignore')
    net.fit(X_train, y_train)
    
    y_pred = net.predict(X_test)
    accuracy = (y_pred == y_test).mean()

    net_weights = net.module_.get_weights()
    total_num = 0
    non_zero_num = 0
    for _, w in net_weights.items():
        total_num += w.numel()
        non_zero_num += (w > thr).sum()
    compress_ratio = total_num / non_zero_num
    print(accuracy, compress_ratio)
    return {'accuracy': accuracy,
            'compress_ratio': compress_ratio,
            'weights': net_weights}

train_and_compress_ratio(FNN, max_epochs=10, weight_decay=1e-2)

train_and_compress_ratio(SparseWeightNet, max_epochs=10, weight_decay=1e-2)

class SparseLinear2(nn.Module):
    def __init__(
            self,
            input_dim=784,
            output_dim=10,
    ):
        super(SparseLinear2, self).__init__()

        self.output = RedLinear(input_dim, output_dim)

    def forward(self, X, **kwargs):
        #X = (X *lf.input_mask)
        #X = self.dropout(X)
        X = F.softmax(self.output(X), dim=-1)
        return X

torch.manual_seed(0)

net2 = NeuralNetClassifier(
    SparseLinear2,
    max_epochs=20,
    lr=1,
    device=device,
    optimizer=torch.optim.SGD,
    optimizer__weight_decay=0.0005
)

warnings.filterwarnings('ignore')

net2.fit(X_train, y_train);

m = net2.module_.output.hidden.data.abs().shape

print(m[0] * m[1])
print((net2.module_.output.hidden.data.abs() < 1e-3).sum())

weight = (net2.module_.output.hidden * net2.module_.output.hidden2).data.view(28, 28, 10).abs().detach().cpu()
for i in range(10):
  
  plt.imshow(weight[:,:,i])
  plt.colorbar()
  plt.show()

  plt.close()

"""# 20 News"""

# Commented out IPython magic to ensure Python compatibility.
#### 20news



import timeit
import warnings

import matplotlib.pyplot as plt
import numpy as np

from sklearn.datasets import fetch_20newsgroups_vectorized
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.exceptions import ConvergenceWarning

warnings.filterwarnings("ignore", category=ConvergenceWarning, module="sklearn")
t0 = timeit.default_timer()

# We use SAGA solver
solver = "saga"

# Turn down for faster run time
n_samples = 5000

X, y = fetch_20newsgroups_vectorized(subset="all", return_X_y=True)

X = X.astype('float32')
y = y.astype('int64')
X = X[:n_samples]
y = y[:n_samples]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=42, stratify=y, test_size=0.1
)
train_samples, n_features = X_train.shape
n_classes = np.unique(y).shape[0]

print(
    "Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i"
#     % (train_samples, n_features, n_classes)
)

models = {
    "ovr": {"name": "One versus Rest", "iters": [2]},
    "multinomial": {"name": "Multinomial", "iters": [2]},
}

for model in models:
    # Add initial chance-level values for plotting purpose
    accuracies = [1 / n_classes]
    times = [0]
    densities = [1]

    model_params = models[model]

    # Small number of epochs for fast runtime
    for this_max_iter in model_params["iters"]:
        print(
            "[model=%s, solver=%s] Number of epochs: %s"
#             % (model_params["name"], solver, this_max_iter)
        )
        lr = LogisticRegression(
            solver=solver,
            multi_class=model,
            penalty="l1",
            max_iter=this_max_iter,
            random_state=42,
            C=1
        )
        t1 = timeit.default_timer()
        lr.fit(X_train, y_train)
        train_time = timeit.default_timer() - t1

        y_pred = lr.predict(X_test)
        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]
        density = np.mean(lr.coef_ != 0, axis=1) * 100
        accuracies.append(accuracy)
        densities.append(density)
        times.append(train_time)
    models[model]["times"] = times
    models[model]["densities"] = densities
    models[model]["accuracies"] = accuracies
    print("Test accuracy for model %s: %.4f" % (model, accuracies[-1]))
    print(
        "%% non-zero coefficients for model %s, per class:\n %s"
#         % (model, densities[-1])
    )
    print(
        "Run time (%i epochs) for model %s:%.2f"
#         % (model_params["iters"][-1], model, times[-1])
    )

fig = plt.figure()
ax = fig.add_subplot(111)

for model in models:
    name = models[model]["name"]
    times = models[model]["times"]
    accuracies = models[model]["accuracies"]
    ax.plot(times, accuracies, marker="o", label="Model: %s" % name)
    ax.set_xlabel("Train time (s)")
    ax.set_ylabel("Test accuracy")
ax.legend()
fig.suptitle("Multinomial vs One-vs-Rest Logistic L1\nDataset %s" % "20newsgroups")
fig.tight_layout()
fig.subplots_adjust(top=0.85)
run_time = timeit.default_timer() - t0
print("Example run in %.3f s" % run_time)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import timeit
import warnings

import matplotlib.pyplot as plt
import numpy as np

from sklearn.datasets import fetch_20newsgroups_vectorized
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.exceptions import ConvergenceWarning

warnings.filterwarnings("ignore", category=ConvergenceWarning, module="sklearn")
t0 = timeit.default_timer()

# We use SAGA solver
solver = "saga"

# Turn down for faster run time

X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=42, stratify=y, test_size=0.1
)
train_samples, n_features = X_train.shape
n_classes = np.unique(y).shape[0]

print(
    "Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i"
#     % (train_samples, n_features, n_classes)
)

models = {
    "ovr": {"name": "One versus Rest", "iters": [1, 2]},
    "multinomial": {"name": "Multinomial", "iters": [1,2]},
}

for model in models:
    # Add initial chance-level values for plotting purpose
    accuracies = [1 / n_classes]
    times = [0]
    densities = [1]

    model_params = models[model]

    # Small number of epochs for fast runtime
    for this_max_iter in model_params["iters"]:
        print(
            "[model=%s, solver=%s] Number of epochs: %s"
#             % (model_params["name"], solver, this_max_iter)
        )
        lr = LogisticRegression(
            solver=solver,
            multi_class=model,
            penalty="l1",
            max_iter=this_max_iter,
            random_state=42,
            C=1
        )
        t1 = timeit.default_timer()
        lr.fit(X_train, y_train)
        train_time = timeit.default_timer() - t1

        y_pred = lr.predict(X_test)
        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]
        density = np.mean(lr.coef_ != 0, axis=1) * 100
        accuracies.append(accuracy)
        densities.append(density)
        times.append(train_time)
    models[model]["times"] = times
    models[model]["densities"] = densities
    models[model]["accuracies"] = accuracies
    print("Test accuracy for model %s: %.4f" % (model, accuracies[-1]))
    print(
        "%% non-zero coefficients for model %s, per class:\n %s"
#         % (model, densities[-1])
    )
    print(
        "Run time (%i epochs) for model %s:%.2f"
#         % (model_params["iters"][-1], model, times[-1])
    )

fig = plt.figure()
ax = fig.add_subplot(111)

for model in models:
    name = models[model]["name"]
    times = models[model]["times"]
    accuracies = models[model]["accuracies"]
    ax.plot(times, accuracies, marker="o", label="Model: %s" % name)
    ax.set_xlabel("Train time (s)")
    ax.set_ylabel("Test accuracy")
ax.legend()
fig.suptitle("Multinomial vs One-vs-Rest Logistic L1\nDataset %s" % "20newsgroups")
fig.tight_layout()
fig.subplots_adjust(top=0.85)
run_time = timeit.default_timer() - t0
print("Example run in %.3f s" % run_time)
plt.show()



class SparseLinear(nn.Module):
    def __init__(
            self,
            input_dim=130107,
            output_dim=20,
    ):
        super(SparseLinear, self).__init__()
        self.input_mask = nn.Parameter(torch.zeros([1, input_dim]).normal_(0, 1))
        #self.hidden = nn.Linear(input_dim, hidden_dim)
        self.output = nn.Linear(input_dim, output_dim)

    def forward(self, X, **kwargs):
        X = (X * self.input_mask)
        #X = self.dropout(X)
        X = F.softmax(self.output(X), dim=-1)
        return X

class SparseLinear2(nn.Module):
    def __init__(
            self,
            input_dim=130107,
            output_dim=20,
    ):
        super(SparseLinear2, self).__init__()

        self.output = RedLinear(input_dim, output_dim)

    def forward(self, X, **kwargs):
        #X = (X *lf.input_mask)
        #X = self.dropout(X)
        X = F.softmax(self.output(X), dim=-1)
        return X


class RedLinear(nn.Module):
    def __init__(self,in_dim,out_dim):
        super(RedLinear, self).__init__()
        self.hidden = nn.Parameter(torch.zeros([in_dim, out_dim]).normal_(0, 1/out_dim ** 0.25))
        self.hidden2 = nn.Parameter(torch.zeros([in_dim, out_dim]).normal_(0, 1/out_dim ** 0.25))

        self.bias = nn.Parameter(torch.zeros([1, out_dim]).normal_(0, 1))


    def forward(self, X, **kwargs):
        X = (X).mm((self.hidden * self.hidden2)) + self.bias
        #print(X.shape)
        #X = self.dropout(X)
        #X = F.softmax(self.output(X), dim=-1)
        return X



class SparseNet1(nn.Module):
    def __init__(
            self,
            input_dim=130107,
            hidden_dim=200,
            output_dim=20,
            dropout=0.5,
    ):
        super(SparseNet1, self).__init__()
        self.dropout = nn.Dropout(dropout)

        self.hidden = RedLinear(input_dim, hidden_dim)
        self.output = RedLinear(hidden_dim, output_dim)

    def forward(self, X, **kwargs):
        X = F.relu(self.hidden(X))
        #X = self.dropout(X)
        X = F.softmax(self.output(X), dim=-1)
        return X

class FNN(nn.Module):
    def __init__(
            self,
            input_dim=130107,
            hidden_dim=200,
            output_dim=20,
    ):
        super(FNN, self).__init__()

        self.hidden = nn.Linear(input_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, output_dim)

    def forward(self, X, **kwargs):
        X = F.relu(self.hidden(X))
        #X = self.dropout(X)
        X = F.softmax(self.output(X), dim=-1)
        return X

torch.manual_seed(0)

net = NeuralNetClassifier(
    FNN,
    max_epochs=50,
    lr=4e-3,
    device=device,
    optimizer=torch.optim.Adam,
    optimizer__weight_decay=0
)

warnings.filterwarnings('ignore')

net.fit(X_train, y_train);

torch.manual_seed(0)

net2 = NeuralNetClassifier(
    SparseNet1,
    max_epochs=50,
    lr=4e-3,
    device=device,
    optimizer=torch.optim.Adam,
    optimizer__weight_decay=1e-7
    #optimizer__momentum=0.9
)

warnings.filterwarnings('ignore')
net2.fit(X_train, y_train);

m = net2.module_.hidden.hidden.data.abs().shape

print(m[0] * m[1])
print((net2.module_.hidden.hidden.data.abs() < 1e-4).sum())

y_pred = net2.predict(X_test)
accuracy = np.sum(y_pred == y_test) / y_test.shape[0]
print(accuracy)

print(density)

"""## Prediction"""

from sklearn.metrics import accuracy_score

y_pred = net.predict(X_test)

accuracy_score(y_test, y_pred)

"""An accuracy of about 96% for a network with only one hidden layer is not too bad.

Let's take a look at some predictions that went wrong:
"""

error_mask = y_pred != y_test

plot_example(X_test[error_mask], y_pred[error_mask])

"""# Convolutional Network
PyTorch expects a 4 dimensional tensor as input for its 2D convolution layer. The dimensions represent:
* Batch size
* Number of channel
* Height
* Width

As initial batch size the number of examples needs to be provided. MNIST data has only one channel. As stated above, each MNIST vector represents a 28x28 pixel image. Hence, the resulting shape for PyTorch tensor needs to be (x, 1, 28, 28). 
"""

XCnn = X.reshape(-1, 1, 28, 28)

XCnn.shape

XCnn_train, XCnn_test, y_train, y_test = train_test_split(XCnn, y, test_size=0.25, random_state=42)

XCnn_train.shape, y_train.shape

class Cnn(nn.Module):
    def __init__(self, dropout=0.5):
        super(Cnn, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.conv2_drop = nn.Dropout2d(p=dropout)
        self.fc1 = nn.Linear(1600, 100) # 1600 = number channels * width * height
        self.fc2 = nn.Linear(100, 10)
        self.fc1_drop = nn.Dropout(p=dropout)

    def forward(self, x):
        x = torch.relu(F.max_pool2d(self.conv1(x), 2))
        x = torch.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        
        # flatten over channel, height and width = 1600
        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))
        
        x = torch.relu(self.fc1_drop(self.fc1(x)))
        x = torch.softmax(self.fc2(x), dim=-1)
        return x

torch.manual_seed(0)

cnn = NeuralNetClassifier(
    Cnn,
    max_epochs=10,
    lr=0.002,
    optimizer=torch.optim.Adam,
    device=device,
)

cnn.fit(XCnn_train, y_train);

y_pred_cnn = cnn.predict(XCnn_test)

accuracy_score(y_test, y_pred_cnn)

"""An accuracy of >98% should suffice for this example!

Let's see how we fare on the examples that went wrong before:
"""

accuracy_score(y_test[error_mask], y_pred_cnn[error_mask])

"""Over 70% of the previously misclassified images are now correctly identified."""

plot_example(X_test[error_mask], y_pred_cnn[error_mask])